{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5d401-eb87-4152-bfa9-e0fd23c66031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=== HORIZON EUROPE MODEL RESULTS ANALYSIS ===\")\n",
    "\n",
    "# 1. Colour Settings\n",
    "SOFT_COLORS = ['#7B9DB8', '#A8C5B0', '#D4B896']\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 10,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 14\n",
    "})\n",
    "\n",
    "# 2. File Location\n",
    "plots_dir = r\"D:\\\\KU Leuven\\\\Stats\\\\MDA\\\\horizon-funding\\\\data\\\\plots\"\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "    print(f\" Created directory: {plots_dir}\")\n",
    "\n",
    "# 3. Load the Models Results\n",
    "try:\n",
    "    rf_results = catalog.load(\"rf_test_metrics\")\n",
    "    xgb_results = catalog.load(\"xgb_test_metrics\")\n",
    "    logistic_results = catalog.load(\"logistic_test_metrics\")\n",
    "    \n",
    "    results = {\n",
    "        'Random Forest': rf_results,\n",
    "        'XGBoost': xgb_results,\n",
    "        'Logistic Regression': logistic_results\n",
    "    }\n",
    "    print(\" All model results loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error loading results: {e}\")\n",
    "\n",
    "# 4. Show Performance Summary\n",
    "print(\"\\n=== MODEL PERFORMANCE SUMMARY ===\")\n",
    "summary_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{model_results['accuracy']:.4f}\",\n",
    "        'F1 Score': f\"{model_results['f1']:.4f}\",\n",
    "        'Precision': f\"{model_results['precision']:.4f}\",\n",
    "        'Recall': f\"{model_results['recall']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# 5. Generate performance comparison chart \n",
    "models = list(results.keys())\n",
    "accuracies = [results[model]['accuracy'] for model in models]\n",
    "f1_scores = [results[model]['f1'] for model in models]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('Model Performance Comparison - Horizon Europe', \n",
    "             fontsize=16, fontweight='bold', y=0.95) \n",
    "\n",
    "# Accuracy\n",
    "bars1 = axes[0].bar(models, accuracies, color=SOFT_COLORS, alpha=0.8, \n",
    "                    edgecolor='white', linewidth=2)\n",
    "axes[0].set_title('Accuracy Comparison', fontweight='bold', pad=20)  \n",
    "axes[0].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[0].set_ylim(0, 1.1)  \n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=15)  \n",
    "\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# F1-Score\n",
    "bars2 = axes[1].bar(models, f1_scores, color=SOFT_COLORS, alpha=0.8, \n",
    "                    edgecolor='white', linewidth=2)\n",
    "axes[1].set_title('F1 Score Comparison', fontweight='bold', pad=20) \n",
    "axes[1].set_ylabel('F1 Score', fontweight='bold')\n",
    "axes[1].set_ylim(0, 1.1) \n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=15) \n",
    "\n",
    "for bar, f1 in zip(bars2, f1_scores):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Layout\n",
    "plt.subplots_adjust(left=0.08, right=0.95, top=0.85, bottom=0.15, wspace=0.25)\n",
    "plt.savefig(f\"{plots_dir}/model_performance_simple.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\" Performance plot saved to: {plots_dir}/model_performance_simple.png\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Generate confusion matrix \n",
    "class_labels = ['Small\\n(≤€2M)', 'Medium\\n(€2-4M)', 'Large\\n(>€4M)'] \n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "fig.suptitle('Confusion Matrices', fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "for i, (model_name, model_results) in enumerate(results.items()):\n",
    "    if 'confusion_matrix' in model_results:\n",
    "        cm = np.array(model_results['confusion_matrix'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=class_labels, yticklabels=class_labels,\n",
    "                   ax=axes[i], cbar_kws={'shrink': 0.8},\n",
    "                   linewidths=1, linecolor='white',\n",
    "                   annot_kws={'fontsize': 11, 'fontweight': 'bold'})\n",
    "        \n",
    "        axes[i].set_title(f'{model_name}', fontweight='bold', pad=20) \n",
    "        axes[i].set_xlabel('Predicted Class', fontweight='bold', labelpad=10)\n",
    "        axes[i].set_ylabel('True Class', fontweight='bold', labelpad=10)\n",
    "        \n",
    "        \n",
    "        axes[i].tick_params(axis='x', labelsize=9, rotation=0)\n",
    "        axes[i].tick_params(axis='y', labelsize=9, rotation=0)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.06, right=0.98, top=0.85, bottom=0.15, wspace=0.3)\n",
    "plt.savefig(f\"{plots_dir}/confusion_matrices_simple.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\" Confusion matrices saved to: {plots_dir}/confusion_matrices_simple.png\")\n",
    "plt.show()\n",
    "\n",
    "# 7. Select the best\n",
    "best_model = max(results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"\\n BEST PERFORMING MODEL: {best_model[0]}\")\n",
    "print(f\"   F1 Score: {best_model[1]['f1']:.4f}\")\n",
    "print(f\"   Accuracy: {best_model[1]['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n All files saved to: {plots_dir}\")\n",
    "print(\" Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aaa892-77ea-4264-ae52-0ad80ea80fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (horizon_funding)",
   "language": "python",
   "name": "kedro_horizon_funding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
