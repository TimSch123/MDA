{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=== ENHANCED HORIZON EUROPE MODEL SELECTION ANALYSIS ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Enhanced Color Settings and Style\n",
    "SOFT_COLORS = ['#7B9DB8', '#A8C5B0', '#D4B896']\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 15\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. File Location Setup\n",
    "plots_dir = r\"D:\\\\KU Leuven\\\\Stats\\\\MDA\\\\horizon-funding\\\\data\\\\plots\"\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "    print(\"Created directory: {}\".format(plots_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Load Model Results and Cross-Validation Scores\n",
    "print(\"\\n=== LOADING MODEL RESULTS ===\")\n",
    "try:\n",
    "    # Test metrics\n",
    "    rf_results = catalog.load(\"rf_test_metrics\")\n",
    "    xgb_results = catalog.load(\"xgb_test_metrics\")\n",
    "    logistic_results = catalog.load(\"logistic_test_metrics\")\n",
    "    \n",
    "    # Cross-validation scores for statistical analysis\n",
    "    rf_cv_scores = catalog.load(\"rf_cv_scores\")\n",
    "    xgb_cv_scores = catalog.load(\"xgb_cv_scores\")\n",
    "    logistic_cv_scores = catalog.load(\"logistic_cv_scores\")\n",
    "    \n",
    "    results = {\n",
    "        'Random Forest': rf_results,\n",
    "        'XGBoost': xgb_results,\n",
    "        'Logistic Regression': logistic_results\n",
    "    }\n",
    "    \n",
    "    cv_scores = {\n",
    "        'Random Forest': rf_cv_scores,\n",
    "        'XGBoost': xgb_cv_scores,\n",
    "        'Logistic Regression': logistic_cv_scores\n",
    "    }\n",
    "    \n",
    "    print(\"All model results and CV scores loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error loading results: {}\".format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Comprehensive Performance Summary with Statistical Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create detailed performance comparison\n",
    "detailed_summary = []\n",
    "for model_name, model_results in results.items():\n",
    "    cv_mean = np.mean(cv_scores[model_name]) if model_name in cv_scores else 0\n",
    "    cv_std = np.std(cv_scores[model_name]) if model_name in cv_scores else 0\n",
    "    \n",
    "    detailed_summary.append({\n",
    "        'Model': model_name,\n",
    "        'Test_Accuracy': model_results['accuracy'],\n",
    "        'Test_F1': model_results['f1'],\n",
    "        'Test_Precision': model_results['precision'],\n",
    "        'Test_Recall': model_results['recall'],\n",
    "        'CV_Mean': cv_mean,\n",
    "        'CV_Std': cv_std,\n",
    "        'CV_Stability': 1 - (cv_std / cv_mean) if cv_mean > 0 else 0\n",
    "    })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_summary)\n",
    "\n",
    "# Display formatted results\n",
    "print(\"\\nDETAILED PERFORMANCE METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in detailed_df.iterrows():\n",
    "    print(\"{:18s} | Acc: {:.4f} | F1: {:.4f} | Prec: {:.4f} | Rec: {:.4f}\".format(\n",
    "        row['Model'], row['Test_Accuracy'], row['Test_F1'], row['Test_Precision'], row['Test_Recall']))\n",
    "    print(\"{:18s} | CV: {:.4f}±{:.4f} | Stability: {:.4f}\".format(\n",
    "        \"\", row['CV_Mean'], row['CV_Std'], row['CV_Stability']))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Statistical Significance Testing\n",
    "print(\"\\n=== STATISTICAL SIGNIFICANCE ANALYSIS ===\")\n",
    "if len(cv_scores) == 3:\n",
    "    models = list(cv_scores.keys())\n",
    "    comparisons = []\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            t_stat, p_value = stats.ttest_rel(cv_scores[model1], cv_scores[model2])\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            comparisons.append({\n",
    "                'Comparison': \"{} vs {}\".format(model1, model2),\n",
    "                'T-statistic': t_stat,\n",
    "                'P-value': p_value,\n",
    "                'Significance': significance\n",
    "            })\n",
    "            \n",
    "            print(\"{} vs {}:\".format(model1, model2))\n",
    "            print(\"  T-statistic: {:.4f}, P-value: {:.4f} ({})\".format(t_stat, p_value, significance))\n",
    "\n",
    "models = list(results.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Figure 1: Performance Metrics Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Performance Metrics Comparison', fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "metric_titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    values = [results[model][metric] for model in models]\n",
    "    \n",
    "    bars = axes[row, col].bar(models, values, color=SOFT_COLORS, alpha=0.8, \n",
    "                             edgecolor='white', linewidth=2)\n",
    "    axes[row, col].set_title(title, fontweight='bold', pad=15)\n",
    "    axes[row, col].set_ylabel('Score', fontweight='bold')\n",
    "    axes[row, col].set_ylim(0, 1.1)\n",
    "    axes[row, col].tick_params(axis='x', rotation=15)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                           '{:.3f}'.format(val), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, hspace=0.3, wspace=0.3)\n",
    "plt.savefig(\"{}/performance_metrics_comparison.png\".format(plots_dir), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(\"Performance metrics plot saved to: {}/performance_metrics_comparison.png\".format(plots_dir))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23497f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Figure 2: Cross-Validation Stability Analysis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cv_means = [np.mean(cv_scores[model]) for model in models]\n",
    "cv_stds = [np.std(cv_scores[model]) for model in models]\n",
    "\n",
    "bars = ax.bar(models, cv_means, yerr=cv_stds, capsize=8, \n",
    "              color=SOFT_COLORS, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax.set_title('Cross-Validation Performance and Stability', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('CV Accuracy', fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01,\n",
    "            '{:.3f}'.format(mean), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}/cv_stability_analysis.png\".format(plots_dir), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(\"CV stability plot saved to: {}/cv_stability_analysis.png\".format(plots_dir))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Figure 3: Confusion Matrices\n",
    "class_labels = ['Small\\n(<=2M EUR)', 'Medium\\n(2-4M EUR)', 'Large\\n(>4M EUR)']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Confusion Matrices for Model Comparison', fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "for idx, (model_name, model_results) in enumerate(results.items()):\n",
    "    if 'confusion_matrix' in model_results:\n",
    "        cm = np.array(model_results['confusion_matrix'])\n",
    "        \n",
    "        cmap = sns.blend_palette(['white', SOFT_COLORS[idx]], as_cmap=True)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
    "                   xticklabels=class_labels, yticklabels=class_labels,\n",
    "                   ax=axes[idx], cbar_kws={'shrink': 0.8},\n",
    "                   linewidths=1, linecolor='white',\n",
    "                   annot_kws={'fontsize': 12, 'fontweight': 'bold'})\n",
    "        \n",
    "        axes[idx].set_title('{}'.format(model_name), fontweight='bold', pad=20)\n",
    "        axes[idx].set_xlabel('Predicted Class', fontweight='bold')\n",
    "        axes[idx].set_ylabel('True Class', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85, wspace=0.3)\n",
    "plt.savefig(\"{}/confusion_matrices_comparison.png\".format(plots_dir), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(\"Confusion matrices plot saved to: {}/confusion_matrices_comparison.png\".format(plots_dir))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Figure 4: Performance Radar Chart\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "radar_metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "angles = np.linspace(0, 2*np.pi, len(radar_metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for i, (model_name, color) in enumerate(zip(models, SOFT_COLORS)):\n",
    "    values = [results[model_name][metric] for metric in radar_metrics]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=4, label=model_name, \n",
    "            color=color, alpha=0.8, markersize=8)\n",
    "    ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([m.capitalize() for m in radar_metrics], fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Overall Performance Radar Chart', fontweight='bold', pad=30, fontsize=16)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}/performance_radar_chart.png\".format(plots_dir), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(\"Performance radar chart saved to: {}/performance_radar_chart.png\".format(plots_dir))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. Advanced Model Selection Logic\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED MODEL SELECTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weighting_schemes = {\n",
    "    'Balanced': {'accuracy': 0.25, 'f1': 0.25, 'precision': 0.25, 'recall': 0.25},\n",
    "    'F1-Focused': {'accuracy': 0.2, 'f1': 0.4, 'precision': 0.2, 'recall': 0.2},\n",
    "    'Precision-Focused': {'accuracy': 0.2, 'f1': 0.2, 'precision': 0.4, 'recall': 0.2},\n",
    "    'Recall-Focused': {'accuracy': 0.2, 'f1': 0.2, 'precision': 0.2, 'recall': 0.4}\n",
    "}\n",
    "\n",
    "selection_results = {}\n",
    "\n",
    "for scheme_name, weights in weighting_schemes.items():\n",
    "    scores = {}\n",
    "    for model_name, model_results in results.items():\n",
    "        composite_score = sum(model_results[metric] * weight \n",
    "                            for metric, weight in weights.items())\n",
    "        stability_bonus = detailed_df[detailed_df['Model'] == model_name]['CV_Stability'].iloc[0] * 0.1\n",
    "        final_score = composite_score + stability_bonus\n",
    "        scores[model_name] = final_score\n",
    "    \n",
    "    best_model = max(scores.items(), key=lambda x: x[1])\n",
    "    selection_results[scheme_name] = best_model\n",
    "    \n",
    "    print(\"\\n{} WEIGHTING SCHEME:\".format(scheme_name.upper()))\n",
    "    print(\"Weights: {}\".format(weights))\n",
    "    for model, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(\"  {:18s}: {:.4f}\".format(model, score))\n",
    "    print(\"Selected: {} (Score: {:.4f})\".format(best_model[0], best_model[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11. Business Context Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUSINESS CONTEXT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nFOR EU FUNDING PREDICTION TASK:\")\n",
    "print(\"\\n1. PRECISION IMPORTANCE:\")\n",
    "print(\"   High precision reduces false positive funding predictions\")\n",
    "print(\"   Prevents overestimation of funding opportunities\")\n",
    "print(\"   Critical for resource allocation planning\")\n",
    "\n",
    "print(\"\\n2. RECALL IMPORTANCE:\")\n",
    "print(\"   High recall ensures funding opportunities are not missed\")\n",
    "print(\"   Important for comprehensive funding landscape analysis\")\n",
    "print(\"   Helps identify all potential funding sources\")\n",
    "\n",
    "print(\"\\n3. MODEL INTERPRETABILITY:\")\n",
    "print(\"   Random Forest: High - Can extract feature importance\")\n",
    "print(\"   XGBoost: Medium - Provides feature importance but complex\")\n",
    "print(\"   Logistic Regression: Highest - Direct coefficient interpretation\")\n",
    "\n",
    "print(\"\\n4. COMPUTATIONAL EFFICIENCY:\")\n",
    "print(\"   Logistic Regression: Fastest training and prediction\")\n",
    "print(\"   Random Forest: Moderate - Parallelizable\")\n",
    "print(\"   XGBoost: Slower but optimized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 12. Final Recommendation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL SELECTION RECOMMENDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_votes = {}\n",
    "for scheme, (model, score) in selection_results.items():\n",
    "    model_votes[model] = model_votes.get(model, 0) + 1\n",
    "\n",
    "most_voted = max(model_votes.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nRECOMMENDED MODEL: {}\".format(most_voted[0]))\n",
    "print(\"Selected by {}/{} weighting schemes\".format(most_voted[1], len(weighting_schemes)))\n",
    "\n",
    "recommended_model = most_voted[0]\n",
    "recommended_results = results[recommended_model]\n",
    "\n",
    "print(\"\\n{} PERFORMANCE SUMMARY:\".format(recommended_model.upper()))\n",
    "print(\"   Test Accuracy: {:.4f}\".format(recommended_results['accuracy']))\n",
    "print(\"   F1 Score: {:.4f}\".format(recommended_results['f1']))\n",
    "print(\"   Precision: {:.4f}\".format(recommended_results['precision']))\n",
    "print(\"   Recall: {:.4f}\".format(recommended_results['recall']))\n",
    "\n",
    "if recommended_model in cv_scores:\n",
    "    cv_mean = np.mean(cv_scores[recommended_model])\n",
    "    cv_std = np.std(cv_scores[recommended_model])\n",
    "    print(\"   Cross-Val Mean: {:.4f} ± {:.4f}\".format(cv_mean, cv_std))\n",
    "\n",
    "print(\"\\nSELECTION RATIONALE:\")\n",
    "if recommended_model == 'Random Forest':\n",
    "    print(\"   Excellent balance of performance across all metrics\")\n",
    "    print(\"   High interpretability through feature importance\")\n",
    "    print(\"   Robust to overfitting through ensemble approach\")\n",
    "    print(\"   Handles mixed data types effectively\")\n",
    "    print(\"   Good stability in cross-validation\")\n",
    "elif recommended_model == 'XGBoost':\n",
    "    print(\"   Superior predictive performance\")\n",
    "    print(\"   Advanced gradient boosting optimization\")\n",
    "    print(\"   Good handling of complex patterns\")\n",
    "    print(\"   Built-in regularization\")\n",
    "elif recommended_model == 'Logistic Regression':\n",
    "    print(\"   Highest interpretability\")\n",
    "    print(\"   Fast training and prediction\")\n",
    "    print(\"   Good baseline performance\")\n",
    "    print(\"   Minimal computational requirements\")\n",
    "\n",
    "print(\"\\nMODEL SELECTION COMPLETE!\")\n",
    "print(\"Ready for deployment in EU funding prediction system\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 13. Export Selection Summary\n",
    "export_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    export_data.append({\n",
    "        'Model': model_name,\n",
    "        'Selected': model_name == recommended_model,\n",
    "        'Accuracy': model_results['accuracy'],\n",
    "        'F1_Score': model_results['f1'],\n",
    "        'Precision': model_results['precision'],\n",
    "        'Recall': model_results['recall'],\n",
    "        'CV_Mean': np.mean(cv_scores.get(model_name, [0])),\n",
    "        'CV_Std': np.std(cv_scores.get(model_name, [0]))\n",
    "    })\n",
    "\n",
    "export_df = pd.DataFrame(export_data)\n",
    "export_path = \"{}/model_selection_summary.csv\".format(plots_dir)\n",
    "export_df.to_csv(export_path, index=False)\n",
    "print(\"\\nSelection summary exported to: {}\".format(export_path))\n",
    "\n",
    "print(\"\\nAll files saved to: {}\".format(plots_dir))\n",
    "print(\"Generated files:\")\n",
    "print(\"  - performance_metrics_comparison.png\")\n",
    "print(\"  - cv_stability_analysis.png\") \n",
    "print(\"  - confusion_matrices_comparison.png\")\n",
    "print(\"  - performance_radar_chart.png\")\n",
    "print(\"  - model_selection_summary.csv\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (horizon_funding)",
   "language": "python",
   "name": "kedro_horizon_funding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
